---
title: "MODEL 3 CLUSTERING TECHNIQUE"
author: "REY P. PENDANG"
date: "2022-12-16"
output:
  pdf_document: default
  html_document: default
---

# Compare the following clustering technique; 
1 K-Means: 

 K-means clustering is the most common partitioning algorithm. K-means reassigns each data in the dataset to only one of the new clusters formed

2 Hierarchical: 

Hierarchical clustering is separating data into groups based on some measure of similarity, finding a way to measure how they’re alike and different, and further narrowing down the data. 

3 Model Based: 

Model-based clustering is a statistical approach to data clustering. The observed (multivariate) data is assumed to have been generated from a finite mixture of component models., without considering the binary output and categorical variables in the dataset.
  
 

```{r}
# Helper packages
 
library(dplyr)         # for data wrangling
library(tidyverse)     # for filtering 
library(readr)         # load dataset
library(bestNormalize) # for normalizing the dataset
library(ggplot2)       # data visualization
library(stringr)       # for string functionality
library(gridExtra)     # for manipulaiting the grid
library(mclust)        # for model-based clustering
library(cluster)       # for general clustering algorithms
library(factoextra)    # for visualizing cluster results

```
 
# Load the data set

The data frame output of data reprocessing converted into to "csv", which will be used for entire project.

```{r}
dt <- read_csv("normalRad.CSV")
View(dt)

head(dt)
```

# Standardizing the Data

Standardizing data in R can be done by using the standard scale () command and adequate values with various columns of data. It would be best to use the min-max normalization and min-max scaling procedures for large or small data, including the standard scale () command.

```{r}
df <- scale(dt[c(3:431)])
```

# Check for null and missing values
Using *sum(is.n())*.function, We can determine if any missing values in our data.

#The result shows either *True* or *False*. If True, omit the missing values using *na.omit()*
  
#[1] FALSE

#Thus, our data has no missing values.
 
```{r}
sum(is.na(df))
```

# 1 K-MEANS

```{r}
kmeans(df, centers = 3, iter.max = 100, nstart = 100)
clusters <- kmeans(df, centers = 3, iter.max = 100, nstart = 100)
```


# DETERMINING AND VISUALIZING OPTIMAL NUMBER OF CLUSTERS
```{r}
set.seed(123) # Determining Optimal Number of Clusters

fviz_nbclust(df, kmeans, method = "wss") 
fviz_nbclust(df, kmeans, method = "silhouette")
fviz_nbclust(df, kmeans, method = "gap_stat") 

clusters <- kmeans(df, centers = 2, iter.max = 100, nstart = 100)
fviz_cluster(kmeans(df, centers = 2, iter.max = 100, nstart = 100), data = df)
```


# Quality of k-means partition

```{r}
clusters$betweenss / clusters$totss
```


# Visualizing clusters using original variables

```{r}
clusters <- kmeans(df, centers = 3, iter.max = 100, nstart = 100)
dt <- dt |> mutate(cluster = clusters$cluster)
dt |> ggplot(aes(x = Failure, y = Entropy_cooc.W.ADC, col = as.factor(cluster))) + geom_point()
```

# 2 Hierarchical

```{r}
dts <- dt%>%
  select_if(is.numeric) %>%  # select numeric columns
  select(-Failure.binary) %>%    # remove target column
  mutate_all(as.double) %>%  # coerce to double type
  scale()
data <- dist(dts, method = "euclidean")

```

# Completing Linkage

```{r}
ct1 <- hclust(data, method = "complete")
plot(ct1, cex = 0.6)
rect.hclust(ct1, k = 2, border = 1:4)
```

# Computing maximum linkage clustering with agnes
```{r}
set.seed(123)
ct2 <- agnes(dts, method = "complete")
ct2$ac
```
# Computing divisive hierarchical clustering

```{r}
ct3 <- diana(dts)
```

# Divise coefficient

```{r}
ct3$dc
``` 

# Plotting cluster results

```{r}
plot1 <- fviz_nbclust(dts, FUN = hcut, method = "wss", 
                   k.max = 10) +
  ggtitle("(A) Elbow method")
plot2 <- fviz_nbclust(dts, FUN = hcut, method = "silhouette", 
                   k.max = 10) +
  ggtitle("(B) Silhouette method")
plot3 <- fviz_nbclust(dts, FUN = hcut, method = "gap_stat", 
                   k.max = 10) +
  ggtitle("(C) Gap statistic")
```

# Display the plot side by side

```{r}
gridExtra::grid.arrange(plot1, plot2, plot3, nrow = 1)
```

# Ward's Method

```{r}
ct4 <- hclust(data, method = "ward.D2" )

#  Cuting tree into 4 groups

sub_grp <- cutree(ct4, k = 8)

# Number of members in each cluster

table(sub_grp)
```


# 3 Model Based

# Applying gmm model with 3 components
```{r}

model1 <- Mclust(df[,1:10], G=3) 
summary(model1)
model2 = Mclust(df, 1:9) 

summary(model2)

```
Thus, this shows 3 optimal number of clusters with BIC -2632.206. A negative zone with the highest value indicates the preferred model, In general, the lower the BIC value, the better. Plot the results with BIC, density and uncertainty.

# We Plot the results

```{r}
plot(model1, what = "density")
plot(model1, what = "uncertainty")
```


```{r}
legend_args <- list(x = "bottomright", ncol = 5)
plot(model1, what = 'BIC', legendArgs = legend_args)
plot(model1, what = 'classification')
plot(model1, what = 'uncertainty')
```

Plotting  the distribution for all observations. As clusters have more observations with middling levels of probability (i.e., 0.25–0.75), their clusters are usually less compact. Therefore, C3 is less compact than other clusters.

```{r}
probabilities <- model1$z 
colnames(probabilities) <- paste0('C', 1:3)

probabilities <- probabilities %>%
  as.data.frame() %>%
  mutate(id = row_number()) %>%
  tidyr::gather(cluster, probability, -id)

ggplot(probabilities, aes(probability)) +
  geom_histogram() +
  facet_wrap(~ cluster, nrow = 2)
```

Ploting our  the observations that are aligned to each cluster but their uncertainty of membership is greater than 0.25.

```{r}
uncertainty <- data.frame(
  id = 1:nrow(df),
  cluster = model1$classification,
  uncertainty = model1$uncertainty
)
uncertainty %>%
  group_by(cluster) %>%
  filter(uncertainty > 0.25) %>%
  ggplot(aes(uncertainty, reorder(id, uncertainty))) +
  geom_point() +
  facet_wrap(~ cluster, scales = 'free_y', nrow = 1)
```

# Ploting the average standardized consumption for cluster 2 observations compared to all observations.

```{r}
clusterdt<- df %>%
  scale() %>%
  as.data.frame() %>%
  mutate(cluster = model1$classification) %>%
  filter(cluster == 2) %>%
  select(-cluster)

clusterdt%>%
  tidyr::gather(product, std_count) %>%
  group_by(product) %>%
  summarize(avg = mean(std_count)) %>%
  ggplot(aes(avg, reorder(product, avg))) +
  geom_point() +
  labs(x = "Average standardized consumption", y = NULL)
```

# Conclusion of the data set

Thus, using k-means clustering 2 is the best number of clusters with SSwithin = 33.2%. In Hierarchical, gap statistics suggest 9 clusters with 84.90% ac and 84.29%. Lastly, lastly, the model-based suggested 3 optimal number of clusters with BIC -2632.206.

