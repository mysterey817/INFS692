---
title: "MODEL 1 RANDOM FOREST"
author: "REY P. PENDANG"
date: "2022-16-12"
output:
  pdf_document: default
  html_document: default
---

# LOAD PACKAGES

```{r}
# Helper packages
library(readr)    # loading dataset
library(dplyr)    # for data wrangling
library(ggplot2)  # for awesome graphics
library(tidyverse)# for filtering 
library(rsample)  # for creating validation splits
library(bestNormalize) # for normalizing the dataset
library(stringr)       # for string functionality
library(gridExtra)     # for manipulaiting the grid

# Modeling packages
library(cluster)       # for general clustering algorithms
library(factoextra)    # for visualizing cluster results
library(ranger)     # a c++ implementation of random forest 
library(h2o)        # a java-based implementation of random forest
h2o.init()
```

# REPROCESSING DATA

```{r}
dt<- read_csv("radiomics_completedata.csv")
View(dt)
head(dt)
```

# Check for null and missing values
Using *sum(is.n())*.function, We can determine if any missing values in our data.

anyNA(rawd)

#The result shows either *True* or *False*. If True, omit the missing values using *na.omit()*
  
#[1] FALSE

#Thus, our data has no missing values.
 
```{r}
sum(is.na(dt))
```

```{r,warning=F}
dts=dt%>%select_if(is.numeric)
dts=dts[,-1]
dt2=apply(dts,2,function(x){ks.test(x,"pnorm")})
```

To have the list of p-value of all variables, the *unlist()* function is used and convert a list to vector.

```{r}
KS_list=unlist(lapply(dt2, function(x) x$p.value))
```


```{r}
sum(KS_list<0.05) # not normally distributed

sum(KS_list>0.05) # normally distributed
```
# [1] 428
# [1] 1

#  Thus, we have 428 variables that are not normally distributed and Entropy_cooc.W.ADC is normally distributed.


```{r}
which.max(KS_list)
```

# Check for Normality of the Data
We used *Shapiro-Wilk's Test* to check the normality of the data.


```{r,warning=F}
temdt=dt[,c(3,5:length(names(dt)))]

temdt=apply(temdt,2,orderNorm)
temdt=lapply(temdt, function(x) x$x.t)
temdt=temdt%>%as.data.frame()
test=apply(temdt,2,shapiro.test)
test=unlist(lapply(test, function(x) x$p.value))
```

```{r,warning=F}
sum(test>0.05) # not normally distributed
```


```{r,warning=F}
sum(test<0.05) # not normally distributed
```

#[1] 0
#[1] 428

# Thus, base on the result above our data is normally distributed.

```{r}
dt[,c(3,5:length(names(dt)))]=temdt
```

# Getting the correlation of the whole data expect the categorical variables

```{r}
CorMatrix=cor(dt[,-c(1,2)])
heatmap(CorMatrix,Rowv=NA,Colv=NA,scale="none",revC = T)
```

# Split the data into training (80%) and testing (20%)

```{r}
dt$Institution=as.factor(dt$Institution)
dt$Failure.binary=as.factor(dt$Failure.binary)
```

```{r}
splitter <- sample(1:nrow(dt), round(nrow(dt) * 0.8))
traindt <- dt[splitter, ]
testdt  <- dt[-splitter, ]
```


# MODEL 1 RANDOM FOREST

Random forest is a way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of overcoming over-fitting problem of individual decision tree.

```{r}
# Load Packages

library(ROCR)
library(pROC)
library(ranger)  
library(h2o)      # a java-based implementation of random forest
h2o.init()
```

# LOAD THE DATASET

The data frame output of data reprocessing converted into to "csv", which will be used for entire project.

```{r}
# make bootstrapping reproducible

set.seed(123)  # for reproducibility

radiomicsdt<- read_csv("RAD. NORMAL DATA.CSV")
radiomicsdt$Failure.binary=as.factor(radiomicsdt$Failure.binary)

split <- initial_split(radiomicsdt, strata = "Failure.binary")
traindt <- training(split)
testdt <- testing(split)


# number of features
no.features <- length(setdiff(names(traindt), "Failure.binary"))
```


```{r}
# train a default random forest model
randomforest1 <- ranger(
  Failure.binary ~ ., 
  data = traindt,
  mtry = floor(no.features / 3),
  respect.unordered.factors = "order",
  seed = 123
)
```


```{r}
# get OOB RMSE
(default_rmse <- sqrt(randomforest1$prediction.error))

# create hyperparameter grid
hyper_grid <- expand.grid(
  mtry = floor(no.features * c(.05, .15, .25, .333, .4)),
  min.node.size = c(1, 3, 5, 10), 
  replace = c(TRUE, FALSE),                               
  sample.fraction = c(.5, .63, .8),                       
  rmse = NA                                               
)

# execute full cartesian grid search
for(i in seq_len(nrow(hyper_grid))) {
  # fit model for ith hyperparameter combination
  fit <- ranger(
    formula         = Failure.binary ~ ., 
    data            = traindt, 
    num.trees       = no.features * 10,
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$min.node.size[i],
    replace         = hyper_grid$replace[i],
    sample.fraction = hyper_grid$sample.fraction[i],
    verbose         = FALSE,
    seed            = 123,
    respect.unordered.factors = 'order',
  )
  # export OOB error 
  hyper_grid$rmse[i] <- sqrt(fit$prediction.error)
}
```


```{r}
# assess top 10 models
hyper_grid %>%
  arrange(rmse) %>%
  mutate(perc_gain = (default_rmse - rmse) / default_rmse * 100) %>%
  head(10)

h2o.no_progress()
h2o.init(max_mem_size = "5g")
```


```{r}
# convert training data to h2o object
train_h2o <- as.h2o(traindt)

# set the response column to Failure.binary
response <- "Failure.binary"

# set the predictor names
predictors <- setdiff(colnames(traindt), response)

h2o_rf1 <- h2o.randomForest(
  x = predictors, 
  y = response,
  training_frame = train_h2o, 
  ntrees = no.features * 10,
  seed = 123
)

h2o_rf1
```


```{r}
# hyperparameter grid
hyper_grid <- list(
  mtries = floor(no.features * c(.05, .15, .25, .333, .4)),
  min_rows = c(1, 3, 5, 10),
  max_depth = c(10, 20, 30),
  sample_rate = c(.55, .632, .70, .80)
)

# random grid search strategy
search_criteria <- list(
  strategy = "RandomDiscrete",
  stopping_metric = "mse",
  stopping_tolerance = 0.001,   # stop if improvement is < 0.1%
  stopping_rounds = 10,         # over the last 10 models
  max_runtime_secs = 60*5      # or stop search after 5 min.
)
```


```{r}
# perform grid search 
random_grid <- h2o.grid(
  algorithm = "randomForest",
  grid_id = "rf_random_grid",
  x = predictors, 
  y = response, 
  training_frame = train_h2o,
  hyper_params = hyper_grid,
  ntrees = no.features * 10,
  seed = 123,
  stopping_metric = "RMSE",   
  stopping_rounds = 10,           # stop if last 10 trees added 
  stopping_tolerance = 0.005,     # don't improve RMSE by 0.5%
  search_criteria = search_criteria
)
```


```{r}
# collect the results and sort by our model performance metric 
# of choice
random_grid_perf <- h2o.getGrid(
  grid_id = "rf_random_grid", 
  sort_by = "mse", 
  decreasing = FALSE
)
random_grid_perf
```


```{r}
# re-run model with impurity-based variable importance
rf_impurity <- ranger(
  formula = Failure.binary ~ ., 
  data = traindt, 
  num.trees = 2000,
  mtry = 32,
  min.node.size = 1,
  sample.fraction = .80,
  replace = FALSE,
  importance = "impurity",
  respect.unordered.factors = "order",
  verbose = FALSE,
  seed  = 123
)
```


```{r}
# re-run model with permutation-based variable importance
rf_permutation <- ranger(
  formula = Failure.binary ~ ., 
  data = traindt, 
  num.trees = 2000,
  mtry = 32,
  min.node.size = 1,
  sample.fraction = .80,
  replace = FALSE,
  importance = "permutation",
  respect.unordered.factors = "order",
  verbose = FALSE,
  seed  = 123
)
```


```{r}
p1 <- vip::vip(rf_impurity, num_features = 25, bar = FALSE)
p2 <- vip::vip(rf_permutation, num_features = 25, bar = FALSE)

gridExtra::grid.arrange(p1, p2, nrow = 1)


```

```{r}
# Compute predicted probabilities on training data
m1_prob <- predict(h2o_rf1, train_h2o, type = "prob")
m1_prob=as.data.frame(m1_prob)[,2]
train_h2o=as.data.frame(train_h2o)
# Compute AUC metrics for cv_model1,2 and 3 
perf1 <- prediction(m1_prob,train_h2o$Failure.binary) %>%
  performance(measure = "tpr", x.measure = "fpr")


# Plot ROC curves for cv_model1,2 and 3 
plot(perf1, col = "black", lty = 2)


# ROC plot for training data
roc( train_h2o$Failure.binary ~ m1_prob, plot=TRUE, legacy.axes=FALSE, 
    percent=TRUE, col="black", lwd=2, print.auc=TRUE)



# #Feature Interpretation
# vip(cv_model3, num_features = 20)

# Compute predicted probabilities on training data
test_h2o=as.h2o(testdt)

m2_prob <- predict(h2o_rf1, test_h2o, type = "prob")

m2_prob=as.data.frame(m2_prob)[,2]

test_h2o=as.data.frame(test_h2o)

# Compute AUC metrics for cv_model1,2 and 3 
perf2 <- prediction(m2_prob,test_h2o$Failure.binary) %>%
  performance(measure = "tpr", x.measure = "fpr")


# Plot ROC curves for cv_model1,2 and 3 
plot(perf2, col = "black", lty = 2)


# ROC plot for training data
roc( test_h2o$Failure.binary ~ m2_prob, plot=TRUE, legacy.axes=FALSE, 
    percent=TRUE, col="black", lwd=2, print.auc=TRUE)

```